{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_Tester.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarmientoj24/EE298/blob/master/ANN_Tester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB19X0ns1uAJ",
        "colab_type": "code",
        "outputId": "cb840954-6bff-4fdc-eeb9-66a9003d8c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "!chmod 600 '/content/drive/My Drive/pubg/train/train_V2.csv'\n",
        "!chmod 600 '/content/drive/My Drive/pubg/test/test_V2.csv'\n",
        "INPUT_DIR = '/content/drive/My Drive/pubg'\n",
        "\n",
        "# Specific imports\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "But9gOOh1yVN",
        "colab_type": "code",
        "outputId": "53e18cfd-9eab-4fa8-974f-17de23674e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Helper Functions\n",
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    #start_mem = df.memory_usage().sum() / 1024**2\n",
        "    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    return df\n",
        "\n",
        "def reload():\n",
        "  print(\"Building dataframe...\")\n",
        "  gc.collect()\n",
        "  df = pd.read_csv(INPUT_DIR + '/train/train_V2.csv') # <=========== Just a function to reduce memory usage\n",
        "\n",
        "  # Only take the samples with matches that have more than 1 player \n",
        "  # there are matches with no players or just one player ( those samples could affect our model badly) \n",
        "  df = df[df['maxPlace'] > 1]\n",
        "  invalid_match_ids = df[df['winPlacePerc'].isna()]['matchId'].values\n",
        "  df = df[-df['matchId'].isin(invalid_match_ids)]\n",
        "  print(\"Done loading train to dataframe...\")\n",
        "  return df\n",
        "\n",
        "def train_test_split(df, test_size=0.1):\n",
        "  match_ids = df['matchId'].unique().tolist()\n",
        "  train_size = int(len(match_ids) * (1 - test_size))\n",
        "  train_match_ids = random.sample(match_ids, train_size)\n",
        "\n",
        "  train = df[df['matchId'].isin(train_match_ids)]\n",
        "  test = df[-df['matchId'].isin(train_match_ids)]\n",
        "\n",
        "  return train, test\n",
        "  \n",
        "# Split train to train and eval set\n",
        "def generate_train_test_set(df, split):\n",
        "  print(\"Generating train and test set...\")\n",
        "  # df.drop(columns=['matchType'], inplace=True)\n",
        "  \n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType']\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train, val = train_test_split(df, split)\n",
        "  \n",
        "  return train[cols_to_fit], val[cols_to_fit]\n",
        "\n",
        "def generate_train_set(df):\n",
        "  print(\"Generating train and test set...\")\n",
        "  # df.drop(columns=['matchType'], inplace=True)\n",
        "  \n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType']\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train = df\n",
        "  \n",
        "  return train[cols_to_fit]\n",
        "\n",
        "def load_test():\n",
        "  print(\"Building dataframe...\")\n",
        "  df = pd.read_csv(INPUT_DIR + '/test/test_V2.csv') # <=========== Just a function to reduce memory usage\n",
        "\n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType']\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  print(\"Done loading train to dataframe...\")\n",
        "  return df[cols_to_fit]\n",
        "\n",
        "def transform_preds(df_test, pred):\n",
        "  for i in range(len(df_test)):\n",
        "      winPlacePerc_m = pred[i]\n",
        "      maxPlace = int(df_test.iloc[i]['maxPlace'])\n",
        "      if maxPlace == 0:\n",
        "          winPlacePerc_m = 0.0\n",
        "      elif maxPlace == 1:\n",
        "          winPlacePerc_m = 1.0\n",
        "      else:\n",
        "          gap = 1.0 / (maxPlace - 1)\n",
        "          winPlacePerc_m = np.round(winPlacePerc_m / gap) * gap\n",
        "\n",
        "      if winPlacePerc_m < 0: winPlacePerc_m = 0.0\n",
        "      if winPlacePerc_m > 1: winPlacePerc_m = 1.0    \n",
        "      pred[i] = winPlacePerc_m\n",
        "\n",
        "      if (i + 1) % 100000 == 0:\n",
        "          print(i, flush=True, end=\" \")\n",
        "\n",
        "  df_test['winPlacePerc_mod'] = pred\n",
        "  return df_test\n",
        "\n",
        "def save_for_submission(df, path):\n",
        "  submission = df[['Id', 'winPlacePerc']]\n",
        "  submission.to_csv(path + 'submission.csv', index=False)\n",
        "  \n",
        "# Feature Engineering\n",
        "# Helper Functions\n",
        "\n",
        "def get_playersJoined(df):\n",
        "  df['playersJoined'] = df.groupby('matchId')['matchId'].transform('count')\n",
        "  return df\n",
        "\n",
        "def get_killsNorm(df):\n",
        "  if 'playersJoined' not in df.columns:\n",
        "    df = get_playersJoined(df)\n",
        "  df['killsNorm'] = df['kills']*((100-df['playersJoined'])/100 + 1)\n",
        "  return df\n",
        "\n",
        "def get_damageDealtNorm(df):\n",
        "  if 'playersJoined' not in df.columns:\n",
        "    df = get_playersJoined(df)\n",
        "  df['damageDealtNorm'] = df['damageDealt']*((100-df['playersJoined'])/100 + 1)\n",
        "  return df\n",
        "\n",
        "def get_healsAndBoosts(df):\n",
        "  df['healsAndBoosts'] = df['heals'] + df['boosts']\n",
        "  return df\n",
        "\n",
        "def get_totalDistance(df):\n",
        "  df['totalDistance'] = df['walkDistance']+ df['rideDistance']+ df['swimDistance']\n",
        "  return df\n",
        "\n",
        "def get_team(df):\n",
        "  df['team'] = [1 if i>50 else 2 if (i>25 & i<=50) else 4 for i in df['numGroups']]\n",
        "  return df\n",
        "\n",
        "def get_players_in_team(df):\n",
        "    agg = df.groupby(['groupId']).size().to_frame('players_in_team')\n",
        "    return df.merge(agg, how='left', on=['groupId'])\n",
        "\n",
        "def get_headshotKills_over_kills(df):\n",
        "  df['headshotKills_over_kills'] = df['headshotKills'] / df['kills']\n",
        "  df['headshotKills_over_kills'].fillna(0, inplace=True)\n",
        "  return df\n",
        "\n",
        "def get_killPlace_over_maxPlace(df):\n",
        "    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n",
        "    df['killPlace_over_maxPlace'].fillna(0, inplace=True)\n",
        "    df['killPlace_over_maxPlace'].replace(np.inf, 0, inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_walkDistance_over_heals(df):\n",
        "    df['walkDistance_over_heals'] = df['walkDistance'] / df['heals']\n",
        "    df['walkDistance_over_heals'].fillna(0, inplace=True)\n",
        "    df['walkDistance_over_heals'].replace(np.inf, 0, inplace=True)\n",
        "    return df\n",
        "  \n",
        "def get_walkDistance_over_boosts(df):\n",
        "    df['walkDistance_over_boosts'] = df['walkDistance'] / df['boosts']\n",
        "    df['walkDistance_over_boosts'].fillna(0, inplace=True)\n",
        "    df['walkDistance_over_boosts'].replace(np.inf, 0, inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_walkDistance_over_kills(df):\n",
        "    df['walkDistance_over_kills'] = df['walkDistance'] / df['kills']\n",
        "    df['walkDistance_over_kills'].fillna(0, inplace=True)\n",
        "    df['walkDistance_over_kills'].replace(np.inf, 0, inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_teamwork(df):\n",
        "    df['teamwork'] = df['assists'] + df['revives']\n",
        "    return df\n",
        "\n",
        "# BY AGGREGATES, meaning, they calculate mean/max/min, etc of each columns then add it into the left of the existing one\n",
        "def add_min_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId','groupId'])[features].min()\n",
        "    return df.merge(agg, suffixes=['', '_min'], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "def add_max_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].max()\n",
        "    return df.merge(agg, suffixes=['', '_max'], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "def add_sum_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].sum()\n",
        "    return df.merge(agg, suffixes=['', '_sum'], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "def add_median_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].median()\n",
        "    return df.merge(agg, suffixes=['', '_median'], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "def add_mean_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n",
        "    return df.merge(agg, suffixes=['', '_mean'], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "def add_rank_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n",
        "    agg = agg.groupby('matchId')[features].rank(pct=True)\n",
        "    return df.merge(agg, suffixes=['', '_mean_rank'], how='left', on=['matchId', 'groupId'])\n",
        "  \n",
        "# Build the NEURAL NET\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, BatchNormalization, PReLU, LeakyReLU\n",
        "from keras.models import load_model\n",
        "from keras import optimizers, regularizers\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# adjusted because im still testing so i reduced hidden units from 64 to 32, epoch reduced\n",
        "class NNModel():\n",
        "  # network parameters\n",
        "  batch_size = 32\n",
        "  hidden_units = 64\n",
        "  dropout = 0.2\n",
        "  optimizer = 'adam'\n",
        "  \n",
        "  def __init__(self, input_size, reload=False, path=None):\n",
        "    if reload:\n",
        "      self.model = load_model(path)\n",
        "    else:\n",
        "      # Regression has 1 output layer\n",
        "      output_shape = 1\n",
        "\n",
        "      self.model = model = Sequential()\n",
        "      self.model.add(Dense(self.hidden_units, input_dim=input_size, init=\"glorot_uniform\", bias_initializer='zeros'))\n",
        "      # self.model.add(LeakyReLU(alpha=self.leaky_relu))\n",
        "      self.model.add(Activation('sigmoid'))\n",
        "      self.model.add(Dropout(self.dropout))\n",
        "      self.model.add(Dense(self.hidden_units, init=\"glorot_uniform\", bias_initializer='zeros'))\n",
        "      self.model.add(Activation('sigmoid'))\n",
        "      # self.model.add(LeakyReLU(alpha=self.leaky_relu))\n",
        "      self.model.add(Dropout(self.dropout))\n",
        "      self.model.add(Dense(output_shape))\n",
        "      # this is the output for one-hot vector\n",
        "      self.model.add(Activation('linear'))\n",
        " \n",
        "  def _summarize(self):\n",
        "    self.model.summary()\n",
        "  \n",
        "  def _compile(self):\n",
        "    self.model.compile(loss='mse',\n",
        "              optimizer=self.optimizer,\n",
        "              metrics=['mse', 'mae'])\n",
        "    \n",
        "  def _train(self, x_train, y_train, epochs):\n",
        "    filepath=\"/content/drive/My Drive/pubg/epochs:{epoch:03d}-mse:{mean_squared_error:.3f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='mean_squared_error', verbose=1, \n",
        "                                 save_best_only=True, mode='min', period=2)\n",
        "    callbacks_list = [checkpoint]\n",
        "    \n",
        "    self.history = self.model.fit(x_train, y_train, \n",
        "              epochs=epochs, batch_size=self.batch_size)\n",
        "    \n",
        "  def _evaluate(self, x_test, y_test):\n",
        "    return self.model.evaluate(x_test, y_test, batch_size=self.batch_size)\n",
        "  \n",
        "  def _predict(self, x_test):\n",
        "    return self.model.predict(x_test)\n",
        "  \n",
        "  # For Plotting Purposes\n",
        "  def _history(self):\n",
        "    return self.history\n",
        "\n",
        "class AdvancedNN(NNModel):\n",
        "  def __init__(self, input_size, reload=False, path=None):\n",
        "    if reload:\n",
        "      self.model = load_model(path)\n",
        "    else:\n",
        "      self.model = Sequential()\n",
        "      self.model.add(Dense(256, init=\"glorot_uniform\", input_dim=input_size, activation='relu'))\n",
        "      self.model.add(BatchNormalization())\n",
        "      self.model.add(Dropout(0.2))\n",
        "\n",
        "      self.model.add(Dense(128, init=\"glorot_uniform\", activation='relu'))\n",
        "      # self.model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n",
        "      self.model.add(BatchNormalization())\n",
        "      self.model.add(Dropout(0.2))\n",
        "\n",
        "      self.model.add(Dense(64, init=\"glorot_uniform\", activation='relu'))\n",
        "      #self.model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n",
        "      self.model.add(BatchNormalization())\n",
        "      self.model.add(Dropout(0.1))\n",
        "      \n",
        "      self.model.add(Dense(1, init=\"glorot_uniform\", activation='linear'))\n",
        "      # self.optimizer = optimizers.Adam(lr=0.005)\n",
        "      self.batch_size = 2**15\n",
        "      # self.lr_sched = self.step_decay_schedule(initial_lr=0.001, decay_factor=0.97, step_size=1, verbose=1)\n",
        "      self.early_stopping = EarlyStopping(monitor='mean_squared_error', mode='min', patience=19, verbose=1)\n",
        "      \n",
        "  def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n",
        "      ''' Wrapper function to create a LearningRateScheduler with step decay schedule. '''\n",
        "      def schedule(epoch):\n",
        "          return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
        "\n",
        "      return LearningRateScheduler(schedule, verbose)\n",
        "    \n",
        "  def _train(self, x_train, y_train, epochs):\n",
        "    filepath=\"/content/drive/My Drive/pubg/epochs:{epoch:03d}-mse:{mean_squared_error:.3f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='mean_squared_error', verbose=1, \n",
        "                                 save_best_only=True, mode='min', period=20)\n",
        "    callbacks_list = [checkpoint]\n",
        "    \n",
        "    self.history = self.model.fit(x_train, y_train, \n",
        "              epochs=epochs, batch_size=self.batch_size, callbacks=[self.early_stopping, checkpoint])\n",
        "\n",
        "  \n",
        "def original(df):\n",
        "  return df\n",
        "\n",
        "def get_these_cols(df, remain):\n",
        "  return df[remain]\n",
        "\n",
        "def remove_these_cols(df, remove_cols):\n",
        "  cols_to_remain = [col for col in df.columns if col not in remove_cols]\n",
        "  return df[cols_to_remain]\n",
        "\n",
        "def put_everything_and_rank(df):\n",
        "  df.drop(columns=['matchType'], inplace=True)\n",
        "  df = get_playersJoined(df)\n",
        "  df = get_killsNorm(df)\n",
        "  df = get_damageDealtNorm(df)\n",
        "  df = get_healsAndBoosts(df)\n",
        "  df = get_totalDistance(df)\n",
        "  df = get_team(df)\n",
        "  df = get_players_in_team(df)\n",
        "  df = get_headshotKills_over_kills(df)\n",
        "  df = get_killPlace_over_maxPlace(df)\n",
        "  df = get_walkDistance_over_heals(df)\n",
        "  df = get_walkDistance_over_boosts(df)\n",
        "  df = get_walkDistance_over_kills(df)\n",
        "  df = get_teamwork(df)\n",
        "  df = add_rank_by_team(df)\n",
        "  return df\n",
        "\n",
        "def new_combination_2(df):\n",
        "  df.drop(columns=['matchType'], inplace=True)\n",
        "  # Just combination 1 with ranking\n",
        "  '''\n",
        "        Value                             Feature\n",
        "  0     506                           killPlace\n",
        "  1     421             totalDistance_mean_rank\n",
        "  2     378                 killPlace_mean_rank\n",
        "  3     377              walkDistance_mean_rank\n",
        "  4     293   killPlace_over_maxPlace_mean_rank\n",
        "  5     257                     kills_mean_rank\n",
        "  6     254             killPlace_over_maxPlace\n",
        "  7     224                        walkDistance\n",
        "  8     197               killStreaks_mean_rank\n",
        "  9     188   walkDistance_over_kills_mean_rank\n",
        "  10    161                    boosts_mean_rank\n",
        "  11    146                 killsNorm_mean_rank\n",
        "  12    142           players_in_team_mean_rank\n",
        "  13    136                       playersJoined\n",
        "  14    135                           killsNorm\n",
        "  15    132                     players_in_team\n",
        "  16    114                       totalDistance\n",
        "  17    108           weaponsAcquired_mean_rank\n",
        "  18    107                       matchDuration\n",
        "  19     90                            maxPlace\n",
        "  20     84                           numGroups\n",
        "  '''\n",
        "  df = get_killPlace_over_maxPlace(df)\n",
        "  df = get_totalDistance(df)\n",
        "  df = get_playersJoined(df)\n",
        "  df = get_players_in_team(df)\n",
        "  df = get_walkDistance_over_kills(df)\n",
        "  df = get_killsNorm(df)\n",
        "  df = get_walkDistance_over_boosts(df)\n",
        "  df = get_healsAndBoosts(df)\n",
        "  df = get_damageDealtNorm(df)\n",
        "  df = add_rank_by_team(df)\n",
        "  \n",
        "  to_remove = ['assists', 'heals', 'walkDistance_over_heals', 'swimDistance', 'teamwork', 'teamKills', 'revives', 'headshotKills_over_kills', 'headshotKills', 'roadKills', 'team', 'vehicleDestroys']\n",
        "  df = remove_these_cols(df, to_remove)\n",
        "  return df\n",
        "\n",
        "\n",
        "def new_combination_4(df):\n",
        "  df.drop(columns=['matchType'], inplace=True)\n",
        "  df = put_everything_and_rank(df)\n",
        "  remaining_cols = ['Id', 'groupId', 'matchId', 'winPlacePerc', 'killPlace', 'killPlace_mean_rank', \n",
        "                    'walkDistance_mean_rank', 'totalDistance_mean_rank',\n",
        "                    'killPlace_over_maxPlace', 'killPlace_over_maxPlace_mean_rank', 'kills_mean_rank',\n",
        "                    'killStreaks_mean_rank', 'walkDistance_over_kills_mean_rank', 'killsNorm_mean_rank',\n",
        "                    'walkDistance_over_boosts_mean_rank', 'weaponsAcquired_mean_rank', 'walkDistance',\n",
        "                    'playersJoined', 'players_in_team_mean_rank', 'boosts_mean_rank', 'players_in_team',\n",
        "                    'killsNorm', 'walkDistance_over_kills', 'matchDuration', 'DBNOs_mean_rank', 'numGroups',\n",
        "                    'longestKill_mean_rank', 'maxPlace']\n",
        "                    \n",
        "  df = get_these_cols(df, remaining_cols)\n",
        "  return df\n",
        "\n",
        "def load_test():\n",
        "  print(\"Loading test...\")\n",
        "  df = reduce_mem_usage(pd.read_csv(INPUT_DIR + '/test/test_V2.csv')) # <=========== Just a function to reduce memory usage\n",
        "  return df\n",
        "\n",
        "\n",
        "# Training & Evaluate Phase\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "train_evaluate = False\n",
        "if train_evaluate:\n",
        "  df = reload()\n",
        "  df = new_combination_2(df)\n",
        "  target = 'winPlacePerc'\n",
        "\n",
        "  train, val = generate_train_test_set(df, 0.1)\n",
        "\n",
        "  del df\n",
        "  gc.collect()\n",
        "\n",
        "  x_train, y_train = train.drop(target, axis=1).to_numpy(), train[target].to_numpy()\n",
        "  x_eval, y_eval = val.drop(target, axis=1).to_numpy(), val[target].to_numpy()\n",
        "\n",
        "  scaler = StandardScaler().fit(x_train)\n",
        "  rescaled_x_train = scaler.transform(x_train)\n",
        "\n",
        "  del x_train\n",
        "  del train\n",
        "  del val\n",
        "  gc.collect()\n",
        "\n",
        "  num_labels = len(x_eval[0])\n",
        "  model1 = AdvancedNN(num_labels)\n",
        "  model1._summarize()\n",
        "  model1._compile()\n",
        "  model1._train(rescaled_x_train, y_train, 140)\n",
        "  # Evalute\n",
        "  rescaled_x_test = scaler.transform(x_eval)\n",
        "  score = model1._evaluate(rescaled_x_test, y_eval)\n",
        "  print(\"\\nMean Squared Error [smaller the better]: %f\" % (score[1]))\n",
        "  \n",
        "else:\n",
        "  df = reload()\n",
        "  df = new_combination_2(df)\n",
        "  target = 'winPlacePerc'\n",
        "  train = generate_train_set(df)\n",
        "  x_train, y_train = train.drop(target, axis=1).to_numpy(), train[target].to_numpy()\n",
        "  \n",
        "  # Garbage collection\n",
        "  del train\n",
        "  del df\n",
        "  gc.collect()\n",
        "  \n",
        "  scaler = MinMaxScaler().fit(x_train)\n",
        "  x_train = scaler.transform(x_train)\n",
        "  \n",
        "  num_labels = len(x_train[0])\n",
        "  model1 = NNModel(num_labels)\n",
        "  model1._summarize()\n",
        "  model1._compile()\n",
        "  model1._train(x_train, y_train, 140)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dataframe...\n",
            "Done loading train to dataframe...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kp1aqnV2Wst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}