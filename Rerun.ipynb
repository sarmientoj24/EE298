{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rerun.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPGMVigDqt3EoeBbv04EbgN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarmientoj24/EE298/blob/master/Rerun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFjKg6e8NTov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "67bba34b-31e6-4c5e-ce9d-2e3afc386cd0"
      },
      "source": [
        "from keras.layers import Lambda, Input, Dense, BatchNormalization, LeakyReLU, GlobalAveragePooling2D, Activation, Conv2D, Conv2DTranspose, Flatten, Dense, Reshape\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "# Main cell to run\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-47ZgzENdmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### HELPER FUNCTIONS\n",
        "import cv2\n",
        "import glob, os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import imageio\n",
        "def read_imgs_to_np_from_folder(img_folder):\n",
        "  cwd = os.getcwd()\n",
        "\n",
        "  # os.chdir(img_folder + '/')\n",
        "\n",
        "  imgs = glob.glob(img_folder + '*.jpg')\n",
        "  # height,width = imageio.imread(imgs[0]).shape[:2]\n",
        "\n",
        "  X = []\n",
        "  count = 0\n",
        "  for f in imgs:\n",
        "    x = imageio.imread(f)\n",
        "\n",
        "    if len(x.shape) == 3 and x.shape[0] == 260 and x.shape[1] == 195 and x.shape[2] == 3:\n",
        "      x = x.astype(np.float32) / 255.0\n",
        "      X.append(x)\n",
        "      del x\n",
        "      count += 1\n",
        "    if count % 3000 == 0:\n",
        "      print(\"Traversed \", count)\n",
        "    \n",
        "  X = np.array(X)\n",
        "  print(X.shape)\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8Q0cAEVNp-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 600 '/content/drive/My Drive/rerun/new_imgs6.tar.xz'\n",
        "!cp '/content/drive/My Drive/rerun/new_imgs6.tar.xz' '/content/data_im/'\n",
        "!tar -xf '/content/data_im/new_imgs6.tar.xz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiLqat4DN61a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4321c163-8c92-4a59-fb46-85d7969a8393"
      },
      "source": [
        "# x_train = read_imgs_to_np_from_folder('/content/new_imgs4/')\n",
        "cwd = os.getcwd()\n",
        "\n",
        "# os.chdir(img_folder + '/')\n",
        "\n",
        "imgs = glob.glob('/content/new_imgs6/' + '*.jpg')\n",
        "# height,width = imageio.imread(imgs[0]).shape[:2]\n",
        "\n",
        "x_train = []\n",
        "count = 0\n",
        "for f in imgs:\n",
        "  x = imageio.imread(f)\n",
        "\n",
        "  if len(x.shape) == 3 and x.shape[0] == 240 and x.shape[1] == 180 and x.shape[2] == 3:\n",
        "    x = x.astype(np.float32) / 255.0\n",
        "    x_train.append(x)\n",
        "    del x\n",
        "    count += 1\n",
        "  if count % 3000 == 0:\n",
        "    print(\"Traversed \", count)\n",
        "  \n",
        "x_train = np.array(x_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traversed  3000\n",
            "Traversed  6000\n",
            "Traversed  9000\n",
            "Traversed  12000\n",
            "Traversed  15000\n",
            "Traversed  18000\n",
            "Traversed  21000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew9FcDrbOHb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 777 '/content/drive/My Drive/rerun/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwSTCjzzPLFh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c3e1fb2-9bc5-4aa7-ef94-ab805050cf9f"
      },
      "source": [
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean=0 and std=1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "z_dim = 16\n",
        "kernel_size = 2\n",
        "latent_dim = 16\n",
        "filter_dims = [16, 32, 64]\n",
        "filter_decoder = [64, 32, 16]\n",
        "image_shape = (240, 180, 3)\n",
        "epochs = 200\n",
        "batch_size = 16\n",
        "from keras import layers, initializers\n",
        "from keras.layers import Dropout\n",
        "\n",
        "inputs = Input(shape=image_shape, name='encoder_input')\n",
        "x = inputs\n",
        "for i in range(2):\n",
        "  filters = filter_dims[i]\n",
        "  x = Conv2D(filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=2,\n",
        "            padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(0.1)(x)\n",
        "\n",
        "# shape info needed to build decoder model\n",
        "shape = K.int_shape(x)\n",
        "\n",
        "# generate latent vector Q(z|X)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128)(x)\n",
        "x = LeakyReLU(0.1)(x)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var',\n",
        "      kernel_initializer=initializers.Constant(0),\n",
        "      bias_initializer=initializers.Constant(0))(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True)\n",
        "\n",
        "# build decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(128)(latent_inputs)\n",
        "x = LeakyReLU(0.1)(x)\n",
        "x = Dense(shape[1] * shape[2] * shape[3])(x)\n",
        "x = LeakyReLU(0.1)(x)\n",
        "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
        "\n",
        "for i in range(2):\n",
        "  filters = filter_decoder[i]\n",
        "  x = Conv2DTranspose(filters=filters,\n",
        "                      kernel_size=kernel_size,\n",
        "                      strides=2,\n",
        "                      padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(0.1)(x)\n",
        "\n",
        "outputs = Conv2DTranspose(filters=3,\n",
        "                          kernel_size=kernel_size,\n",
        "                          activation='sigmoid',\n",
        "                          padding='same',\n",
        "                          name='decoder_output')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True)\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae')\n",
        "vae.summary()\n",
        "\n",
        "reconstruction_loss = mse(K.flatten(inputs),\n",
        "                                           K.flatten(outputs))\n",
        "\n",
        "reconstruction_loss *= image_shape[0] * image_shape[1]\n",
        "kl_loss = 1 + (z_log_var) - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# def vae_loss(y_true, y_pred):\n",
        "#     \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
        "#     recon = K.sum(K.binary_crossentropy(y_true, y_pred))\n",
        "#     kl = 0.5 * K.sum(K.exp(z_log_sigma) + K.square(z_mean) - 1. - z_log_sigma, axis=1)\n",
        "#     return recon + kl\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "history = vae.fit(x_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=1)\n",
        "vae.save('/content/drive/My Drive/rerun/vae_binary_cross_adam_epoch_only_relu_200.h5')\n",
        "encoder.save('/content/drive/My Drive/rerun/encoder_binary_cross_adam_epoch_only_relu_200.h5')\n",
        "decoder.save('/content/drive/My Drive/rerun/decoder_binary_cross_adam_epoch_only_relu_200.h5')\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/My Drive/rerun/model_loss.png')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      (None, 240, 180, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 120, 90, 16)  208         encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 120, 90, 16)  64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 120, 90, 16)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 60, 45, 32)   2080        leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 60, 45, 32)   128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 60, 45, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 86400)        0           leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          11059328    flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 16)           2064        leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 16)           2064        leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "z (Lambda)                      (None, 16)           0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 11,065,936\n",
            "Trainable params: 11,065,840\n",
            "Non-trainable params: 96\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "z_sampling (InputLayer)      (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               2176      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 86400)             11145600  \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 86400)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 60, 45, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 120, 90, 64)       8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 120, 90, 64)       256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 120, 90, 64)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 240, 180, 32)      8224      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 240, 180, 32)      128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 240, 180, 32)      0         \n",
            "_________________________________________________________________\n",
            "decoder_output (Conv2DTransp (None, 240, 180, 3)       387       \n",
            "=================================================================\n",
            "Total params: 11,165,027\n",
            "Trainable params: 11,164,835\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "Model: \"vae\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 240, 180, 3)       0         \n",
            "_________________________________________________________________\n",
            "encoder (Model)              [(None, 16), (None, 16),  11065936  \n",
            "_________________________________________________________________\n",
            "decoder (Model)              (None, 240, 180, 3)       11165027  \n",
            "=================================================================\n",
            "Total params: 22,230,963\n",
            "Trainable params: 22,230,675\n",
            "Non-trainable params: 288\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/200\n",
            "21759/21759 [==============================] - 62s 3ms/step - loss: 844.6348\n",
            "Epoch 2/200\n",
            "21759/21759 [==============================] - 54s 2ms/step - loss: 578.4313\n",
            "Epoch 3/200\n",
            "21759/21759 [==============================] - 54s 2ms/step - loss: 538.1362\n",
            "Epoch 4/200\n",
            "21759/21759 [==============================] - 54s 2ms/step - loss: 516.7053\n",
            "Epoch 5/200\n",
            "21759/21759 [==============================] - 54s 2ms/step - loss: 498.2025\n",
            "Epoch 6/200\n",
            "21759/21759 [==============================] - 54s 2ms/step - loss: 486.5042\n",
            "Epoch 7/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 475.1455\n",
            "Epoch 8/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 464.0966\n",
            "Epoch 9/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 455.5049\n",
            "Epoch 10/200\n",
            "21759/21759 [==============================] - 54s 2ms/step - loss: 447.4945\n",
            "Epoch 11/200\n",
            "21759/21759 [==============================] - 54s 2ms/step - loss: 438.9083\n",
            "Epoch 12/200\n",
            "21759/21759 [==============================] - 54s 2ms/step - loss: 433.5074\n",
            "Epoch 13/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 428.3236\n",
            "Epoch 14/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 421.9255\n",
            "Epoch 15/200\n",
            "21759/21759 [==============================] - 54s 2ms/step - loss: 416.9227\n",
            "Epoch 16/200\n",
            "21759/21759 [==============================] - 54s 2ms/step - loss: 411.9566\n",
            "Epoch 17/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 407.5452\n",
            "Epoch 18/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 403.3085\n",
            "Epoch 19/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 398.7923\n",
            "Epoch 20/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 395.6141\n",
            "Epoch 21/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 392.3744\n",
            "Epoch 22/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 388.5890\n",
            "Epoch 23/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 385.3011\n",
            "Epoch 24/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 382.6028\n",
            "Epoch 25/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 379.5756\n",
            "Epoch 26/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 377.1947\n",
            "Epoch 27/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 374.1320\n",
            "Epoch 28/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 371.4928\n",
            "Epoch 29/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 368.2042\n",
            "Epoch 30/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 366.5515\n",
            "Epoch 31/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 364.8938\n",
            "Epoch 32/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 362.4370\n",
            "Epoch 33/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 359.5336\n",
            "Epoch 34/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 358.2653\n",
            "Epoch 35/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 355.7028\n",
            "Epoch 36/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 355.0906\n",
            "Epoch 37/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 352.1401\n",
            "Epoch 38/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 350.7234\n",
            "Epoch 39/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 348.8240\n",
            "Epoch 40/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 347.9894\n",
            "Epoch 41/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 346.0839\n",
            "Epoch 42/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 345.1688\n",
            "Epoch 43/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 342.2289\n",
            "Epoch 44/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 340.7889\n",
            "Epoch 45/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 340.1898\n",
            "Epoch 46/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 338.9573\n",
            "Epoch 47/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 337.5024\n",
            "Epoch 48/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 336.0562\n",
            "Epoch 49/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 334.3466\n",
            "Epoch 50/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 333.5418\n",
            "Epoch 51/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 332.1590\n",
            "Epoch 52/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 332.7941\n",
            "Epoch 53/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 331.6419\n",
            "Epoch 54/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 329.1318\n",
            "Epoch 55/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 326.3198\n",
            "Epoch 56/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 327.2063\n",
            "Epoch 57/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 325.7497\n",
            "Epoch 58/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 324.6443\n",
            "Epoch 59/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 322.9369\n",
            "Epoch 60/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 322.3659\n",
            "Epoch 61/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 321.6455\n",
            "Epoch 62/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 321.0625\n",
            "Epoch 63/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 320.4071\n",
            "Epoch 64/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 318.9384\n",
            "Epoch 65/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 317.7154\n",
            "Epoch 66/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 316.6119\n",
            "Epoch 67/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 316.5284\n",
            "Epoch 68/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 315.3522\n",
            "Epoch 69/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 314.9146\n",
            "Epoch 70/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 313.9185\n",
            "Epoch 71/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 313.0049\n",
            "Epoch 72/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 312.4480\n",
            "Epoch 73/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 311.5862\n",
            "Epoch 74/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 311.0395\n",
            "Epoch 75/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 310.2194\n",
            "Epoch 76/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 310.2131\n",
            "Epoch 77/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 308.8028\n",
            "Epoch 78/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 308.6928\n",
            "Epoch 79/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 307.5772\n",
            "Epoch 80/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 307.0829\n",
            "Epoch 81/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 306.0650\n",
            "Epoch 82/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 305.8679\n",
            "Epoch 83/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 304.8515\n",
            "Epoch 84/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 304.7838\n",
            "Epoch 85/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 304.2532\n",
            "Epoch 86/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 304.2439\n",
            "Epoch 87/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 302.8865\n",
            "Epoch 88/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 302.2589\n",
            "Epoch 89/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 302.0207\n",
            "Epoch 90/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 301.6271\n",
            "Epoch 91/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 300.3495\n",
            "Epoch 92/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 300.2464\n",
            "Epoch 93/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 299.2714\n",
            "Epoch 94/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 299.4389\n",
            "Epoch 95/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 298.5916\n",
            "Epoch 96/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 298.3137\n",
            "Epoch 97/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 298.5655\n",
            "Epoch 98/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 297.7004\n",
            "Epoch 99/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 296.6718\n",
            "Epoch 100/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 296.0156\n",
            "Epoch 101/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 296.0290\n",
            "Epoch 102/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 295.5356\n",
            "Epoch 103/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 295.0761\n",
            "Epoch 104/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 294.9660\n",
            "Epoch 105/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 294.3947\n",
            "Epoch 106/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 293.5823\n",
            "Epoch 107/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 293.8723\n",
            "Epoch 108/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 292.9050\n",
            "Epoch 109/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 292.5263\n",
            "Epoch 110/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 292.4333\n",
            "Epoch 111/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 292.2451\n",
            "Epoch 112/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 291.1910\n",
            "Epoch 113/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 291.3198\n",
            "Epoch 114/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 291.1697\n",
            "Epoch 115/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 290.5053\n",
            "Epoch 116/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 289.5051\n",
            "Epoch 117/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 289.7135\n",
            "Epoch 118/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 289.4683\n",
            "Epoch 119/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 288.6096\n",
            "Epoch 120/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 288.4830\n",
            "Epoch 121/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 288.3641\n",
            "Epoch 122/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 287.4649\n",
            "Epoch 123/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 287.5252\n",
            "Epoch 124/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 287.0534\n",
            "Epoch 125/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 287.0179\n",
            "Epoch 126/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 286.5889\n",
            "Epoch 127/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 286.3876\n",
            "Epoch 128/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 286.0684\n",
            "Epoch 129/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 285.6335\n",
            "Epoch 130/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 285.0094\n",
            "Epoch 131/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 284.4202\n",
            "Epoch 132/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 285.3658\n",
            "Epoch 133/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 284.3168\n",
            "Epoch 134/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 284.2367\n",
            "Epoch 135/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 284.4368\n",
            "Epoch 136/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 283.3130\n",
            "Epoch 137/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 283.2330\n",
            "Epoch 138/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 283.6069\n",
            "Epoch 139/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 283.2653\n",
            "Epoch 140/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 282.1509\n",
            "Epoch 141/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 282.4864\n",
            "Epoch 142/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 281.8660\n",
            "Epoch 143/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 281.5066\n",
            "Epoch 144/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 281.3851\n",
            "Epoch 145/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 280.8519\n",
            "Epoch 146/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 281.1082\n",
            "Epoch 147/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 280.5812\n",
            "Epoch 148/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 279.9780\n",
            "Epoch 149/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 280.5187\n",
            "Epoch 150/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 279.4707\n",
            "Epoch 151/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 279.3030\n",
            "Epoch 152/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 279.1422\n",
            "Epoch 153/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 278.8643\n",
            "Epoch 154/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 278.5605\n",
            "Epoch 155/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 279.8168\n",
            "Epoch 156/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 277.6407\n",
            "Epoch 157/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 277.4491\n",
            "Epoch 158/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 277.6675\n",
            "Epoch 159/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 277.6011\n",
            "Epoch 160/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 277.4721\n",
            "Epoch 161/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 277.0518\n",
            "Epoch 162/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 277.1976\n",
            "Epoch 163/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 276.3335\n",
            "Epoch 164/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 276.3204\n",
            "Epoch 165/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 275.8525\n",
            "Epoch 166/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 276.4855\n",
            "Epoch 167/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 276.0615\n",
            "Epoch 168/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 274.9424\n",
            "Epoch 169/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 275.2314\n",
            "Epoch 170/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 274.9995\n",
            "Epoch 171/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 275.2701\n",
            "Epoch 172/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 274.7893\n",
            "Epoch 173/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 274.6849\n",
            "Epoch 174/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 277.1965\n",
            "Epoch 175/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 273.2418\n",
            "Epoch 176/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 273.4870\n",
            "Epoch 177/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 274.4276\n",
            "Epoch 178/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 273.8777\n",
            "Epoch 179/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 273.0422\n",
            "Epoch 180/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 273.7221\n",
            "Epoch 181/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 273.1690\n",
            "Epoch 182/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 272.6090\n",
            "Epoch 183/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 272.5100\n",
            "Epoch 184/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 272.3436\n",
            "Epoch 185/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 272.5515\n",
            "Epoch 186/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 271.7333\n",
            "Epoch 187/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 273.2920\n",
            "Epoch 188/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 271.8969\n",
            "Epoch 189/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 270.9016\n",
            "Epoch 190/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 271.0446\n",
            "Epoch 191/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 271.2298\n",
            "Epoch 192/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 271.8684\n",
            "Epoch 193/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 271.1168\n",
            "Epoch 194/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 270.7397\n",
            "Epoch 195/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 270.3298\n",
            "Epoch 196/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 270.3724\n",
            "Epoch 197/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 270.0213\n",
            "Epoch 198/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 270.1537\n",
            "Epoch 199/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 270.0871\n",
            "Epoch 200/200\n",
            "21759/21759 [==============================] - 53s 2ms/step - loss: 270.3790\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3XSJIAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytrYYTg_RDpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}