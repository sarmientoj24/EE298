{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_Tester.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarmientoj24/EE298/blob/master/ANN_Tester_High_ACC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB19X0ns1uAJ",
        "colab_type": "code",
        "outputId": "d30df463-87b4-4dea-99eb-d38a7b91191d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import random\n",
        "# random.seed(42)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "!chmod 600 '/content/drive/My Drive/pubg/train/train_V2.csv'\n",
        "!chmod 600 '/content/drive/My Drive/pubg/test/test_V2.csv'\n",
        "INPUT_DIR = '/content/drive/My Drive/pubg'\n",
        "\n",
        "# Specific imports\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "But9gOOh1yVN",
        "colab_type": "code",
        "outputId": "5d6e3312-907d-4606-940a-e61d261b4fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Helper Functions\n",
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    #start_mem = df.memory_usage().sum() / 1024**2\n",
        "    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    return df\n",
        "\n",
        "def reload():\n",
        "  print(\"Building dataframe...\")\n",
        "  gc.collect()\n",
        "  df = pd.read_csv(INPUT_DIR + '/train/train_V2.csv') # <=========== Just a function to reduce memory usage\n",
        "\n",
        "  # Only take the samples with matches that have more than 1 player \n",
        "  # there are matches with no players or just one player ( those samples could affect our model badly) \n",
        "  df = df[df['maxPlace'] > 1]\n",
        "  invalid_match_ids = df[df['winPlacePerc'].isna()]['matchId'].values\n",
        "  df = df[-df['matchId'].isin(invalid_match_ids)]\n",
        "  print(\"Done loading train to dataframe...\")\n",
        "  return df\n",
        "\n",
        "def train_test_split(df, test_size=0.1):\n",
        "  match_ids = df['matchId'].unique().tolist()\n",
        "  train_size = int(len(match_ids) * (1 - test_size))\n",
        "  train_match_ids = random.sample(match_ids, train_size)\n",
        "\n",
        "  train = df[df['matchId'].isin(train_match_ids)]\n",
        "  test = df[-df['matchId'].isin(train_match_ids)]\n",
        "\n",
        "  return train, test\n",
        "  \n",
        "# Split train to train and eval set\n",
        "def generate_train_test_set(df, split):\n",
        "  print(\"Generating train and test set...\")\n",
        "  # df.drop(columns=['matchType'], inplace=True)\n",
        "  \n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType']\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train, val = train_test_split(df, split)\n",
        "  \n",
        "  return train[cols_to_fit], val[cols_to_fit]\n",
        "\n",
        "def generate_train_set(df):\n",
        "  print(\"Generating train and test set...\")\n",
        "  # df.drop(columns=['matchType'], inplace=True)\n",
        "  \n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType']\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  train = df\n",
        "  \n",
        "  return train[cols_to_fit]\n",
        "\n",
        "def load_test():\n",
        "  print(\"Building dataframe...\")\n",
        "  df = pd.read_csv(INPUT_DIR + '/test/test_V2.csv') # <=========== Just a function to reduce memory usage\n",
        "\n",
        "  cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType']\n",
        "  cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n",
        "  print(\"Done loading train to dataframe...\")\n",
        "  return df[cols_to_fit]\n",
        "\n",
        "def transform_preds(df_test, pred):\n",
        "  for i in range(len(df_test)):\n",
        "      winPlacePerc_m = pred[i]\n",
        "      maxPlace = int(df_test.iloc[i]['maxPlace'])\n",
        "      if maxPlace == 0:\n",
        "          winPlacePerc_m = 0.0\n",
        "      elif maxPlace == 1:\n",
        "          winPlacePerc_m = 1.0\n",
        "      else:\n",
        "          gap = 1.0 / (maxPlace - 1)\n",
        "          winPlacePerc_m = np.round(winPlacePerc_m / gap) * gap\n",
        "\n",
        "      if winPlacePerc_m < 0: winPlacePerc_m = 0.0\n",
        "      if winPlacePerc_m > 1: winPlacePerc_m = 1.0    \n",
        "      pred[i] = winPlacePerc_m\n",
        "\n",
        "      if (i + 1) % 100000 == 0:\n",
        "          print(i, flush=True, end=\" \")\n",
        "\n",
        "  df_test['winPlacePerc_mod'] = pred\n",
        "  return df_test\n",
        "\n",
        "def save_for_submission(df, path):\n",
        "  submission = df[['Id', 'winPlacePerc']]\n",
        "  submission.to_csv(path + 'submission.csv', index=False)\n",
        "  \n",
        "# Feature Engineering\n",
        "# Helper Functions\n",
        "\n",
        "def get_playersJoined(df):\n",
        "  df['playersJoined'] = df.groupby('matchId')['matchId'].transform('count')\n",
        "  return df\n",
        "\n",
        "def get_killsNorm(df):\n",
        "  if 'playersJoined' not in df.columns:\n",
        "    df = get_playersJoined(df)\n",
        "  df['killsNorm'] = df['kills']*((100-df['playersJoined'])/100 + 1)\n",
        "  return df\n",
        "\n",
        "def get_damageDealtNorm(df):\n",
        "  if 'playersJoined' not in df.columns:\n",
        "    df = get_playersJoined(df)\n",
        "  df['damageDealtNorm'] = df['damageDealt']*((100-df['playersJoined'])/100 + 1)\n",
        "  return df\n",
        "\n",
        "def get_healsAndBoosts(df):\n",
        "  df['healsAndBoosts'] = df['heals'] + df['boosts']\n",
        "  return df\n",
        "\n",
        "def get_totalDistance(df):\n",
        "  df['totalDistance'] = df['walkDistance']+ df['rideDistance']+ df['swimDistance']\n",
        "  return df\n",
        "\n",
        "def get_team(df):\n",
        "  df['team'] = [1 if i>50 else 2 if (i>25 & i<=50) else 4 for i in df['numGroups']]\n",
        "  return df\n",
        "\n",
        "def get_players_in_team(df):\n",
        "    agg = df.groupby(['groupId']).size().to_frame('players_in_team')\n",
        "    return df.merge(agg, how='left', on=['groupId'])\n",
        "\n",
        "def get_headshotKills_over_kills(df):\n",
        "  df['headshotKills_over_kills'] = df['headshotKills'] / df['kills']\n",
        "  df['headshotKills_over_kills'].fillna(0, inplace=True)\n",
        "  return df\n",
        "\n",
        "def get_killPlace_over_maxPlace(df):\n",
        "    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n",
        "    df['killPlace_over_maxPlace'].fillna(0, inplace=True)\n",
        "    df['killPlace_over_maxPlace'].replace(np.inf, 0, inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_walkDistance_over_heals(df):\n",
        "    df['walkDistance_over_heals'] = df['walkDistance'] / df['heals']\n",
        "    df['walkDistance_over_heals'].fillna(0, inplace=True)\n",
        "    df['walkDistance_over_heals'].replace(np.inf, 0, inplace=True)\n",
        "    return df\n",
        "  \n",
        "def get_walkDistance_over_boosts(df):\n",
        "    df['walkDistance_over_boosts'] = df['walkDistance'] / df['boosts']\n",
        "    df['walkDistance_over_boosts'].fillna(0, inplace=True)\n",
        "    df['walkDistance_over_boosts'].replace(np.inf, 0, inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_walkDistance_over_kills(df):\n",
        "    df['walkDistance_over_kills'] = df['walkDistance'] / df['kills']\n",
        "    df['walkDistance_over_kills'].fillna(0, inplace=True)\n",
        "    df['walkDistance_over_kills'].replace(np.inf, 0, inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_teamwork(df):\n",
        "    df['teamwork'] = df['assists'] + df['revives']\n",
        "    return df\n",
        "\n",
        "# BY AGGREGATES, meaning, they calculate mean/max/min, etc of each columns then add it into the left of the existing one\n",
        "def add_min_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId','groupId'])[features].min()\n",
        "    return df.merge(agg, suffixes=['', '_min'], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "def add_max_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].max()\n",
        "    return df.merge(agg, suffixes=['', '_max'], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "def add_sum_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].sum()\n",
        "    return df.merge(agg, suffixes=['', '_sum'], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "def add_median_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].median()\n",
        "    return df.merge(agg, suffixes=['', '_median'], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "def add_mean_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n",
        "    return df.merge(agg, suffixes=['', '_mean'], how='left', on=['matchId', 'groupId'])\n",
        "\n",
        "def add_rank_by_team(df):\n",
        "    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n",
        "    features = [col for col in df.columns if col not in cols_to_drop]\n",
        "    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n",
        "    agg = agg.groupby('matchId')[features].rank(pct=True)\n",
        "    return df.merge(agg, suffixes=['', '_mean_rank'], how='left', on=['matchId', 'groupId'])\n",
        "  \n",
        "# Build the NEURAL NET\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, BatchNormalization, PReLU, LeakyReLU\n",
        "from keras.models import load_model\n",
        "from keras import optimizers, regularizers\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# adjusted because im still testing so i reduced hidden units from 64 to 32, epoch reduced\n",
        "class NNModel():\n",
        "  # network parameters\n",
        "  batch_size = 32\n",
        "  hidden_units = 64\n",
        "  dropout = 0.2\n",
        "  optimizer = 'adam'\n",
        "  \n",
        "  def __init__(self, input_size, reload=False, path=None):\n",
        "    if reload:\n",
        "      self.model = load_model(path)\n",
        "    else:\n",
        "      # Regression has 1 output layer\n",
        "      output_shape = 1\n",
        "\n",
        "      self.model = model = Sequential()\n",
        "      self.model.add(Dense(self.hidden_units, input_dim=input_size, init=\"glorot_uniform\", bias_initializer='zeros'))\n",
        "      # self.model.add(LeakyReLU(alpha=self.leaky_relu))\n",
        "      self.model.add(Activation('sigmoid'))\n",
        "      self.model.add(Dropout(self.dropout))\n",
        "      self.model.add(Dense(self.hidden_units, init=\"glorot_uniform\", bias_initializer='zeros'))\n",
        "      self.model.add(Activation('sigmoid'))\n",
        "      # self.model.add(LeakyReLU(alpha=self.leaky_relu))\n",
        "      self.model.add(Dropout(self.dropout))\n",
        "      self.model.add(Dense(output_shape))\n",
        "      # this is the output for one-hot vector\n",
        "      self.model.add(Activation('linear'))\n",
        " \n",
        "  def _summarize(self):\n",
        "    self.model.summary()\n",
        "  \n",
        "  def _compile(self):\n",
        "    self.model.compile(loss='mse',\n",
        "              optimizer=self.optimizer,\n",
        "              metrics=['mse', 'mae'])\n",
        "    \n",
        "  def _train(self, x_train, y_train, epochs):\n",
        "    filepath=\"/content/drive/My Drive/pubg/epochs:{epoch:03d}-mse:{mean_squared_error:.3f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='mean_squared_error', verbose=1, \n",
        "                                 save_best_only=True, mode='min', period=2)\n",
        "    callbacks_list = [checkpoint]\n",
        "    \n",
        "    self.history = self.model.fit(x_train, y_train, \n",
        "              epochs=epochs, batch_size=self.batch_size)\n",
        "    \n",
        "  def _evaluate(self, x_test, y_test):\n",
        "    return self.model.evaluate(x_test, y_test, batch_size=self.batch_size)\n",
        "  \n",
        "  def _predict(self, x_test):\n",
        "    return self.model.predict(x_test)\n",
        "  \n",
        "  # For Plotting Purposes\n",
        "  def _history(self):\n",
        "    return self.history\n",
        "\n",
        "class AdvancedNN(NNModel):\n",
        "  def __init__(self, input_size, reload=False, path=None):\n",
        "    if reload:\n",
        "      self.model = load_model(path)\n",
        "    else:\n",
        "      self.model = Sequential()\n",
        "      self.model.add(Dense(256, init=\"glorot_uniform\", input_dim=input_size, activation='tanh'))\n",
        "      self.model.add(BatchNormalization())\n",
        "      self.model.add(Dropout(0.1))\n",
        "\n",
        "      self.model.add(Dense(128, init=\"glorot_uniform\", activation='tanh'))\n",
        "      # self.model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n",
        "      self.model.add(BatchNormalization())\n",
        "      self.model.add(Dropout(0.1))\n",
        "\n",
        "      self.model.add(Dense(128, init=\"glorot_uniform\", activation='tanh'))\n",
        "      #self.model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n",
        "      self.model.add(BatchNormalization())\n",
        "      self.model.add(Dropout(0.1))\n",
        "      \n",
        "      self.model.add(Dense(64, init=\"glorot_uniform\", activation='tanh'))\n",
        "      #self.model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n",
        "      self.model.add(BatchNormalization())\n",
        "      self.model.add(Dropout(0.1))\n",
        "      \n",
        "      self.model.add(Dense(1, init=\"glorot_uniform\", activation='linear'))\n",
        "      self.optimizer = optimizers.Adam(lr=0.005)\n",
        "      self.batch_size = 2**12\n",
        "      # self.lr_sched = self.step_decay_schedule(initial_lr=0.001, decay_factor=0.97, step_size=1, verbose=1)\n",
        "      self.early_stopping = EarlyStopping(monitor='mean_squared_error', mode='min', patience=20, verbose=1)\n",
        "      \n",
        "  def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n",
        "      ''' Wrapper function to create a LearningRateScheduler with step decay schedule. '''\n",
        "      def schedule(epoch):\n",
        "          return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
        "\n",
        "      return LearningRateScheduler(schedule, verbose)\n",
        "    \n",
        "  def _train(self, x_train, y_train, epochs):\n",
        "    filepath=\"/content/drive/My Drive/pubg/new_feats_epochs:{epoch:03d}-mse:{mean_squared_error:.3f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='mean_squared_error', verbose=1, \n",
        "                                 save_best_only=True, mode='min', period=20)\n",
        "    \n",
        "    learning_rate_reduction = ReduceLROnPlateau(monitor='mean_squared_error', \n",
        "                                            patience=5, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.0001)\n",
        "    callbacks_list = [checkpoint]\n",
        "    \n",
        "    self.history = self.model.fit(x_train, y_train,\n",
        "              epochs=epochs, batch_size=self.batch_size, callbacks=[self.early_stopping, checkpoint, learning_rate_reduction])\n",
        "\n",
        "  \n",
        "def original(df):\n",
        "  return df\n",
        "\n",
        "def get_these_cols(df, remain):\n",
        "  return df[remain]\n",
        "\n",
        "def remove_these_cols(df, remove_cols):\n",
        "  cols_to_remain = [col for col in df.columns if col not in remove_cols]\n",
        "  return df[cols_to_remain]\n",
        "\n",
        "def put_everything_and_rank(df):\n",
        "  df.drop(columns=['matchType'], inplace=True)\n",
        "  df = get_playersJoined(df)\n",
        "  df = get_killsNorm(df)\n",
        "  df = get_damageDealtNorm(df)\n",
        "  df = get_healsAndBoosts(df)\n",
        "  df = get_totalDistance(df)\n",
        "  df = get_team(df)\n",
        "  df = get_players_in_team(df)\n",
        "  df = get_headshotKills_over_kills(df)\n",
        "  df = get_killPlace_over_maxPlace(df)\n",
        "  df = get_walkDistance_over_heals(df)\n",
        "  df = get_walkDistance_over_boosts(df)\n",
        "  df = get_walkDistance_over_kills(df)\n",
        "  df = get_teamwork(df)\n",
        "  df = add_rank_by_team(df)\n",
        "  return df\n",
        "\n",
        "def new_combination_2(df):\n",
        "  df.drop(columns=['matchType'], inplace=True)\n",
        "  # Just combination 1 with ranking\n",
        "  '''\n",
        "        Value                             Feature\n",
        "  0     506                           killPlace\n",
        "  1     421             totalDistance_mean_rank\n",
        "  2     378                 killPlace_mean_rank\n",
        "  3     377              walkDistance_mean_rank\n",
        "  4     293   killPlace_over_maxPlace_mean_rank\n",
        "  5     257                     kills_mean_rank\n",
        "  6     254             killPlace_over_maxPlace\n",
        "  7     224                        walkDistance\n",
        "  8     197               killStreaks_mean_rank\n",
        "  9     188   walkDistance_over_kills_mean_rank\n",
        "  10    161                    boosts_mean_rank\n",
        "  11    146                 killsNorm_mean_rank\n",
        "  12    142           players_in_team_mean_rank\n",
        "  13    136                       playersJoined\n",
        "  14    135                           killsNorm\n",
        "  15    132                     players_in_team\n",
        "  16    114                       totalDistance\n",
        "  17    108           weaponsAcquired_mean_rank\n",
        "  18    107                       matchDuration\n",
        "  19     90                            maxPlace\n",
        "  20     84                           numGroups\n",
        "  '''\n",
        "  df = get_killPlace_over_maxPlace(df)\n",
        "  df = get_totalDistance(df)\n",
        "  df = get_playersJoined(df)\n",
        "  df = get_players_in_team(df)\n",
        "  df = get_walkDistance_over_kills(df)\n",
        "  df = get_killsNorm(df)\n",
        "  df = get_walkDistance_over_boosts(df)\n",
        "  df = get_healsAndBoosts(df)\n",
        "  df = get_damageDealtNorm(df)\n",
        "  df = add_rank_by_team(df)\n",
        "  \n",
        "  to_remove = ['assists', 'heals', 'walkDistance_over_heals', 'swimDistance', 'teamwork', 'teamKills', 'revives', 'headshotKills_over_kills', 'headshotKills', 'roadKills', 'team', 'vehicleDestroys']\n",
        "  df = remove_these_cols(df, to_remove)\n",
        "  return df\n",
        "\n",
        "\n",
        "def new_combination_4(df):\n",
        "  df.drop(columns=['matchType'], inplace=True)\n",
        "  df = put_everything_and_rank(df)\n",
        "  remaining_cols = ['Id', 'groupId', 'matchId', 'winPlacePerc', 'killPlace', 'killPlace_mean_rank', \n",
        "                    'walkDistance_mean_rank', 'totalDistance_mean_rank',\n",
        "                    'killPlace_over_maxPlace', 'killPlace_over_maxPlace_mean_rank', 'kills_mean_rank',\n",
        "                    'killStreaks_mean_rank', 'walkDistance_over_kills_mean_rank', 'killsNorm_mean_rank',\n",
        "                    'walkDistance_over_boosts_mean_rank', 'weaponsAcquired_mean_rank', 'walkDistance',\n",
        "                    'playersJoined', 'players_in_team_mean_rank', 'boosts_mean_rank', 'players_in_team',\n",
        "                    'killsNorm', 'walkDistance_over_kills', 'matchDuration', 'DBNOs_mean_rank', 'numGroups',\n",
        "                    'longestKill_mean_rank', 'maxPlace']\n",
        "                    \n",
        "  df = get_these_cols(df, remaining_cols)\n",
        "  return df\n",
        "\n",
        "def load_test():\n",
        "  print(\"Loading test...\")\n",
        "  df = reduce_mem_usage(pd.read_csv(INPUT_DIR + '/test/test_V2.csv')) # <=========== Just a function to reduce memory usage\n",
        "  return df\n",
        "\n",
        "NUM = 0\n",
        "\n",
        "def feature_engineering(is_train=True):\n",
        "    if is_train: \n",
        "        print(\"processing train.csv\")\n",
        "        df = pd.read_csv(INPUT_DIR + '/train/train_V2.csv')\n",
        "\n",
        "        df = df[df['maxPlace'] > 1]\n",
        "    else:\n",
        "        print(\"processing test.csv\")\n",
        "        df = pd.read_csv(INPUT_DIR + '/test/test_V2.csv')\n",
        "    \n",
        "    # df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
        "    # df = get_playersJoined(df)\n",
        "    df = get_killsNorm(df)\n",
        "    df = get_damageDealtNorm(df)\n",
        "    df = get_healsAndBoosts(df)\n",
        "    df = get_totalDistance(df)\n",
        "    # df = get_team(df)\n",
        "    # df = get_players_in_team(df)\n",
        "    df = get_headshotKills_over_kills(df)\n",
        "    df = get_killPlace_over_maxPlace(df)\n",
        "    df = get_walkDistance_over_heals(df)\n",
        "    df = get_walkDistance_over_boosts(df)\n",
        "    df = get_walkDistance_over_kills(df)\n",
        "    \n",
        "    to_remove = ['assists', 'heals', 'walkDistance_over_heals', 'swimDistance', 'teamwork', 'teamKills', 'revives', 'headshotKills_over_kills', 'headshotKills', 'roadKills', 'team', 'vehicleDestroys']\n",
        "    df = remove_these_cols(df, to_remove)\n",
        "    \n",
        "    if NUM > 0: df = df[:NUM]\n",
        "    \n",
        "    print(\"remove some columns\")\n",
        "    target = 'winPlacePerc'\n",
        "    features = list(df.columns)\n",
        "    features.remove(\"Id\")\n",
        "    features.remove(\"matchId\")\n",
        "    features.remove(\"groupId\")\n",
        "    \n",
        "    features.remove(\"matchType\")\n",
        "    \n",
        "    y = None\n",
        "    \n",
        "    print(\"get target\")\n",
        "    if is_train: \n",
        "        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n",
        "        features.remove(target)\n",
        "    \n",
        "    print(\"get group mean feature\")\n",
        "    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n",
        "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
        "    \n",
        "    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n",
        "    else: df_out = df[['matchId','groupId']]\n",
        "\n",
        "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
        "    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
        "    \n",
        "    print(\"get group max feature\")\n",
        "    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n",
        "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
        "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
        "    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
        "    \n",
        "    print(\"get group min feature\")\n",
        "    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n",
        "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
        "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
        "    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
        "    \n",
        "    print(\"get group size feature\")\n",
        "    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
        "    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n",
        "    \n",
        "    print(\"get match mean feature\")\n",
        "    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n",
        "    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
        "\n",
        "    print(\"get match size feature\")\n",
        "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
        "    df_out = df_out.merge(agg, how='left', on=['matchId'])\n",
        "    \n",
        "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
        "\n",
        "    X = np.array(df_out, dtype=np.float64)\n",
        "    \n",
        "    feature_names = list(df_out.columns)\n",
        "\n",
        "    del df, df_out, agg, agg_rank\n",
        "    gc.collect()\n",
        "\n",
        "    return X, y, feature_names\n",
        "\n",
        "\n",
        "# Training & Evaluate Phase\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "train_evaluate = True\n",
        "if train_evaluate:\n",
        "#   df = reload()\n",
        "#   df = put_everything_and_rank(df)\n",
        "#   target = 'winPlacePerc'\n",
        "\n",
        "#   train, val = generate_train_test_set(df, 0.1)\n",
        "\n",
        "#   del df\n",
        "#   gc.collect()\n",
        "\n",
        "#   x_train, y_train = train.drop(target, axis=1).to_numpy(), train[target].to_numpy()\n",
        "#   x_eval, y_eval = val.drop(target, axis=1).to_numpy(), val[target].to_numpy()\n",
        "\n",
        "#   scaler = StandardScaler().fit(x_train)\n",
        "#   rescaled_x_train = scaler.transform(x_train)\n",
        "\n",
        "#   del x_train\n",
        "#   del train\n",
        "#   del val\n",
        "#   gc.collect()\n",
        "  x_train, y_train, feature_names = feature_engineering(True)\n",
        "  print(feature_names)\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1)).fit(x_train)\n",
        "  x_train = scaler.transform(x_train)\n",
        "\n",
        "  num_labels = len(x_train[0])\n",
        "  model1 = AdvancedNN(num_labels)\n",
        "  model1._summarize()\n",
        "  model1._compile()\n",
        "  model1._train(x_train, y_train, 120)\n",
        "  # Evalute\n",
        "  # rescaled_x_test = scaler.transform(x_eval)\n",
        "  # score = model1._evaluate(rescaled_x_test, y_eval)\n",
        "  # print(\"\\nMean Squared Error [smaller the better]: %f\" % (score[1]))\n",
        "  \n",
        "else:\n",
        "  df = reload()\n",
        "  df = new_combination_2(df)\n",
        "  target = 'winPlacePerc'\n",
        "  train = generate_train_set(df)\n",
        "  x_train, y_train = train.drop(target, axis=1).to_numpy(), train[target].to_numpy()\n",
        "  \n",
        "  # Garbage collection\n",
        "  del train\n",
        "  del df\n",
        "  gc.collect()\n",
        "  \n",
        "  scaler = MinMaxScaler().fit(x_train)\n",
        "  x_train = scaler.transform(x_train)\n",
        "  \n",
        "  num_labels = len(x_train[0])\n",
        "  model1 = NNModel(num_labels)\n",
        "  model1._summarize()\n",
        "  model1._compile()\n",
        "  model1._train(x_train, y_train, 140)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "processing train.csv\n",
            "remove some columns\n",
            "get target\n",
            "get group mean feature\n",
            "get group max feature\n",
            "get group min feature\n",
            "get group size feature\n",
            "get match mean feature\n",
            "get match size feature\n",
            "['boosts_mean', 'damageDealt_mean', 'DBNOs_mean', 'killPlace_mean', 'killPoints_mean', 'kills_mean', 'killStreaks_mean', 'longestKill_mean', 'matchDuration_mean', 'maxPlace_mean', 'numGroups_mean', 'rankPoints_mean', 'rideDistance_mean', 'walkDistance_mean', 'weaponsAcquired_mean', 'winPoints_mean', 'playersJoined_mean', 'killsNorm_mean', 'damageDealtNorm_mean', 'healsAndBoosts_mean', 'totalDistance_mean', 'killPlace_over_maxPlace_mean', 'walkDistance_over_boosts_mean', 'walkDistance_over_kills_mean', 'boosts_mean_rank', 'damageDealt_mean_rank', 'DBNOs_mean_rank', 'killPlace_mean_rank', 'killPoints_mean_rank', 'kills_mean_rank', 'killStreaks_mean_rank', 'longestKill_mean_rank', 'matchDuration_mean_rank', 'maxPlace_mean_rank', 'numGroups_mean_rank', 'rankPoints_mean_rank', 'rideDistance_mean_rank', 'walkDistance_mean_rank', 'weaponsAcquired_mean_rank', 'winPoints_mean_rank', 'playersJoined_mean_rank', 'killsNorm_mean_rank', 'damageDealtNorm_mean_rank', 'healsAndBoosts_mean_rank', 'totalDistance_mean_rank', 'killPlace_over_maxPlace_mean_rank', 'walkDistance_over_boosts_mean_rank', 'walkDistance_over_kills_mean_rank', 'boosts_max', 'damageDealt_max', 'DBNOs_max', 'killPlace_max', 'killPoints_max', 'kills_max', 'killStreaks_max', 'longestKill_max', 'matchDuration_max', 'maxPlace_max', 'numGroups_max', 'rankPoints_max', 'rideDistance_max', 'walkDistance_max', 'weaponsAcquired_max', 'winPoints_max', 'playersJoined_max', 'killsNorm_max', 'damageDealtNorm_max', 'healsAndBoosts_max', 'totalDistance_max', 'killPlace_over_maxPlace_max', 'walkDistance_over_boosts_max', 'walkDistance_over_kills_max', 'boosts_max_rank', 'damageDealt_max_rank', 'DBNOs_max_rank', 'killPlace_max_rank', 'killPoints_max_rank', 'kills_max_rank', 'killStreaks_max_rank', 'longestKill_max_rank', 'matchDuration_max_rank', 'maxPlace_max_rank', 'numGroups_max_rank', 'rankPoints_max_rank', 'rideDistance_max_rank', 'walkDistance_max_rank', 'weaponsAcquired_max_rank', 'winPoints_max_rank', 'playersJoined_max_rank', 'killsNorm_max_rank', 'damageDealtNorm_max_rank', 'healsAndBoosts_max_rank', 'totalDistance_max_rank', 'killPlace_over_maxPlace_max_rank', 'walkDistance_over_boosts_max_rank', 'walkDistance_over_kills_max_rank', 'boosts_min', 'damageDealt_min', 'DBNOs_min', 'killPlace_min', 'killPoints_min', 'kills_min', 'killStreaks_min', 'longestKill_min', 'matchDuration_min', 'maxPlace_min', 'numGroups_min', 'rankPoints_min', 'rideDistance_min', 'walkDistance_min', 'weaponsAcquired_min', 'winPoints_min', 'playersJoined_min', 'killsNorm_min', 'damageDealtNorm_min', 'healsAndBoosts_min', 'totalDistance_min', 'killPlace_over_maxPlace_min', 'walkDistance_over_boosts_min', 'walkDistance_over_kills_min', 'boosts_min_rank', 'damageDealt_min_rank', 'DBNOs_min_rank', 'killPlace_min_rank', 'killPoints_min_rank', 'kills_min_rank', 'killStreaks_min_rank', 'longestKill_min_rank', 'matchDuration_min_rank', 'maxPlace_min_rank', 'numGroups_min_rank', 'rankPoints_min_rank', 'rideDistance_min_rank', 'walkDistance_min_rank', 'weaponsAcquired_min_rank', 'winPoints_min_rank', 'playersJoined_min_rank', 'killsNorm_min_rank', 'damageDealtNorm_min_rank', 'healsAndBoosts_min_rank', 'totalDistance_min_rank', 'killPlace_over_maxPlace_min_rank', 'walkDistance_over_boosts_min_rank', 'walkDistance_over_kills_min_rank', 'group_size', 'boosts', 'damageDealt', 'DBNOs', 'killPlace', 'killPoints', 'kills', 'killStreaks', 'longestKill', 'matchDuration', 'maxPlace', 'numGroups', 'rankPoints', 'rideDistance', 'walkDistance', 'weaponsAcquired', 'winPoints', 'playersJoined', 'killsNorm', 'damageDealtNorm', 'healsAndBoosts', 'totalDistance', 'killPlace_over_maxPlace', 'walkDistance_over_boosts', 'walkDistance_over_kills', 'match_size']\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:286: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, input_dim=170, activation=\"tanh\", kernel_initializer=\"glorot_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:290: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"tanh\", kernel_initializer=\"glorot_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:295: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"tanh\", kernel_initializer=\"glorot_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:300: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"tanh\", kernel_initializer=\"glorot_uniform\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 256)               43776     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 103,809\n",
            "Trainable params: 102,657\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:305: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"linear\", kernel_initializer=\"glorot_uniform\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/120\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "2026744/2026744 [==============================] - 32s 16us/step - loss: 0.1095 - mean_squared_error: 0.1095 - mean_absolute_error: 0.2101\n",
            "Epoch 2/120\n",
            "2026744/2026744 [==============================] - 25s 12us/step - loss: 0.0061 - mean_squared_error: 0.0061 - mean_absolute_error: 0.0599\n",
            "Epoch 3/120\n",
            "2026744/2026744 [==============================] - 25s 12us/step - loss: 0.0040 - mean_squared_error: 0.0040 - mean_absolute_error: 0.0482\n",
            "Epoch 4/120\n",
            "2026744/2026744 [==============================] - 25s 12us/step - loss: 0.0035 - mean_squared_error: 0.0035 - mean_absolute_error: 0.0454\n",
            "Epoch 5/120\n",
            "2026744/2026744 [==============================] - 25s 12us/step - loss: 0.0033 - mean_squared_error: 0.0033 - mean_absolute_error: 0.0439\n",
            "Epoch 6/120\n",
            "2026744/2026744 [==============================] - 25s 12us/step - loss: 0.0032 - mean_squared_error: 0.0032 - mean_absolute_error: 0.0430\n",
            "Epoch 7/120\n",
            "2026744/2026744 [==============================] - 25s 13us/step - loss: 0.0031 - mean_squared_error: 0.0031 - mean_absolute_error: 0.0422\n",
            "Epoch 8/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0030 - mean_squared_error: 0.0030 - mean_absolute_error: 0.0419\n",
            "Epoch 9/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0030 - mean_squared_error: 0.0030 - mean_absolute_error: 0.0412\n",
            "Epoch 10/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0029 - mean_squared_error: 0.0029 - mean_absolute_error: 0.0405\n",
            "Epoch 11/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0028 - mean_squared_error: 0.0028 - mean_absolute_error: 0.0399\n",
            "Epoch 12/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0027 - mean_squared_error: 0.0027 - mean_absolute_error: 0.0391\n",
            "Epoch 13/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0027 - mean_squared_error: 0.0027 - mean_absolute_error: 0.0390\n",
            "Epoch 14/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0026 - mean_squared_error: 0.0026 - mean_absolute_error: 0.0381\n",
            "Epoch 15/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0026 - mean_squared_error: 0.0026 - mean_absolute_error: 0.0380\n",
            "Epoch 16/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0025 - mean_squared_error: 0.0025 - mean_absolute_error: 0.0376\n",
            "Epoch 17/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0025 - mean_squared_error: 0.0025 - mean_absolute_error: 0.0370\n",
            "Epoch 18/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0024 - mean_squared_error: 0.0024 - mean_absolute_error: 0.0368\n",
            "Epoch 19/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0024 - mean_squared_error: 0.0024 - mean_absolute_error: 0.0366\n",
            "Epoch 20/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0024 - mean_squared_error: 0.0024 - mean_absolute_error: 0.0364\n",
            "\n",
            "Epoch 00020: mean_squared_error improved from inf to 0.00239, saving model to /content/drive/My Drive/pubg/new_feats_epochs:020-mse:0.002.hdf5\n",
            "Epoch 21/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0024 - mean_squared_error: 0.0024 - mean_absolute_error: 0.0362\n",
            "Epoch 22/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0023 - mean_squared_error: 0.0023 - mean_absolute_error: 0.0359\n",
            "Epoch 23/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0023 - mean_squared_error: 0.0023 - mean_absolute_error: 0.0357\n",
            "Epoch 24/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0023 - mean_squared_error: 0.0023 - mean_absolute_error: 0.0352\n",
            "Epoch 25/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0023 - mean_squared_error: 0.0023 - mean_absolute_error: 0.0351\n",
            "Epoch 26/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0022 - mean_squared_error: 0.0022 - mean_absolute_error: 0.0350\n",
            "Epoch 27/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0022 - mean_squared_error: 0.0022 - mean_absolute_error: 0.0347\n",
            "Epoch 28/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0022 - mean_squared_error: 0.0022 - mean_absolute_error: 0.0346\n",
            "Epoch 29/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0022 - mean_squared_error: 0.0022 - mean_absolute_error: 0.0343\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "Epoch 30/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0021 - mean_squared_error: 0.0021 - mean_absolute_error: 0.0335\n",
            "Epoch 31/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0021 - mean_squared_error: 0.0021 - mean_absolute_error: 0.0335\n",
            "Epoch 32/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0021 - mean_squared_error: 0.0021 - mean_absolute_error: 0.0333\n",
            "Epoch 33/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0021 - mean_squared_error: 0.0021 - mean_absolute_error: 0.0333\n",
            "Epoch 34/120\n",
            "2026744/2026744 [==============================] - 27s 13us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0331\n",
            "Epoch 35/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0333\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "Epoch 36/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0327\n",
            "Epoch 37/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0327\n",
            "Epoch 38/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0326\n",
            "Epoch 39/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0326\n",
            "Epoch 40/120\n",
            "2026744/2026744 [==============================] - 23s 11us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0325\n",
            "\n",
            "Epoch 00040: mean_squared_error improved from 0.00239 to 0.00197, saving model to /content/drive/My Drive/pubg/new_feats_epochs:040-mse:0.002.hdf5\n",
            "Epoch 41/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0324\n",
            "Epoch 42/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0324\n",
            "Epoch 43/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0324\n",
            "Epoch 44/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0020 - mean_squared_error: 0.0020 - mean_absolute_error: 0.0324\n",
            "Epoch 45/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0323\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "Epoch 46/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0321\n",
            "Epoch 47/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0320\n",
            "Epoch 48/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0320\n",
            "Epoch 49/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0320\n",
            "Epoch 50/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0320\n",
            "\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "Epoch 51/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0318\n",
            "Epoch 52/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0318\n",
            "Epoch 53/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0318\n",
            "Epoch 54/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0318\n",
            "Epoch 55/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0318\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "Epoch 56/120\n",
            "2026744/2026744 [==============================] - 23s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0317\n",
            "Epoch 57/120\n",
            "2026744/2026744 [==============================] - 24s 12us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0317\n",
            "Epoch 58/120\n",
            "2026744/2026744 [==============================] - 23s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0317\n",
            "Epoch 59/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0317\n",
            "Epoch 60/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0317\n",
            "\n",
            "Epoch 00060: mean_squared_error improved from 0.00197 to 0.00188, saving model to /content/drive/My Drive/pubg/new_feats_epochs:060-mse:0.002.hdf5\n",
            "\n",
            "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 61/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0317\n",
            "Epoch 62/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 63/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 64/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0317\n",
            "Epoch 65/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 66/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 67/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 68/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 69/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 70/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 71/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 72/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 73/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 74/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 75/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 76/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 77/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 78/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 79/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 80/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "\n",
            "Epoch 00080: mean_squared_error improved from 0.00188 to 0.00187, saving model to /content/drive/My Drive/pubg/new_feats_epochs:080-mse:0.002.hdf5\n",
            "Epoch 81/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 82/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 83/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 84/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 85/120\n",
            "2026744/2026744 [==============================] - 22s 11us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 86/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 87/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 88/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 89/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 90/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 91/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 92/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 93/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 94/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 95/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 96/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 97/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 98/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 99/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 100/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "\n",
            "Epoch 00100: mean_squared_error improved from 0.00187 to 0.00186, saving model to /content/drive/My Drive/pubg/new_feats_epochs:100-mse:0.002.hdf5\n",
            "Epoch 101/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 102/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 103/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0316\n",
            "Epoch 104/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 105/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 106/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 107/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 108/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 109/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 110/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 111/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 112/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 113/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 114/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 115/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 116/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 117/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 118/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 119/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "Epoch 120/120\n",
            "2026744/2026744 [==============================] - 21s 10us/step - loss: 0.0019 - mean_squared_error: 0.0019 - mean_absolute_error: 0.0315\n",
            "\n",
            "Epoch 00120: mean_squared_error improved from 0.00186 to 0.00186, saving model to /content/drive/My Drive/pubg/new_feats_epochs:120-mse:0.002.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kp1aqnV2Wst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "cded148d-d3eb-4c47-9076-f18066e182fd"
      },
      "source": [
        "def transform_preds(df_test, pred):\n",
        "  for i in range(len(df_test)):\n",
        "      winPlacePerc_m = pred[i]\n",
        "      maxPlace = int(df_test.iloc[i]['maxPlace'])\n",
        "      if maxPlace == 0:\n",
        "          winPlacePerc_m = 0.0\n",
        "      elif maxPlace == 1:\n",
        "          winPlacePerc_m = 1.0\n",
        "      else:\n",
        "          gap = 1.0 / (maxPlace - 1)\n",
        "          winPlacePerc_m = np.round(winPlacePerc_m / gap) * gap\n",
        "\n",
        "      if winPlacePerc_m < 0: winPlacePerc_m = 0.0\n",
        "      if winPlacePerc_m > 1: winPlacePerc_m = 1.0    \n",
        "      pred[i] = winPlacePerc_m\n",
        "\n",
        "      if (i + 1) % 100000 == 0:\n",
        "          print(i, flush=True, end=\" \")\n",
        "\n",
        "  df_test['winPlacePerc'] = pred\n",
        "  return df_test\n",
        "\n",
        "def feature_engineering(is_train=True):\n",
        "    if is_train: \n",
        "        print(\"processing train.csv\")\n",
        "        df = pd.read_csv(INPUT_DIR + '/train/train_V2.csv')\n",
        "\n",
        "        df = df[df['maxPlace'] > 1]\n",
        "    else:\n",
        "        print(\"processing test.csv\")\n",
        "        df = pd.read_csv(INPUT_DIR + '/test/test_V2.csv')\n",
        "    \n",
        "    # df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
        "    # df = get_playersJoined(df)\n",
        "    df = get_killsNorm(df)\n",
        "    df = get_damageDealtNorm(df)\n",
        "    df = get_healsAndBoosts(df)\n",
        "    df = get_totalDistance(df)\n",
        "    # df = get_team(df)\n",
        "    # df = get_players_in_team(df)\n",
        "    df = get_headshotKills_over_kills(df)\n",
        "    df = get_killPlace_over_maxPlace(df)\n",
        "    df = get_walkDistance_over_heals(df)\n",
        "    df = get_walkDistance_over_boosts(df)\n",
        "    df = get_walkDistance_over_kills(df)\n",
        "    \n",
        "    to_remove = ['assists', 'heals', 'walkDistance_over_heals', 'swimDistance', 'teamwork', 'teamKills', 'revives', 'headshotKills_over_kills', 'headshotKills', 'roadKills', 'team', 'vehicleDestroys']\n",
        "    df = remove_these_cols(df, to_remove)\n",
        "    \n",
        "    \n",
        "    print(\"remove some columns\")\n",
        "    target = 'winPlacePerc'\n",
        "    features = list(df.columns)\n",
        "    features.remove(\"Id\")\n",
        "    features.remove(\"matchId\")\n",
        "    features.remove(\"groupId\")\n",
        "    \n",
        "    features.remove(\"matchType\")\n",
        "    \n",
        "    y = None\n",
        "    \n",
        "    print(\"get target\")\n",
        "    if is_train: \n",
        "        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n",
        "        features.remove(target)\n",
        "    \n",
        "    print(\"get group mean feature\")\n",
        "    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n",
        "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
        "    \n",
        "    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n",
        "    else: df_out = df[['Id', 'matchId','groupId']]\n",
        "\n",
        "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
        "    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
        "    \n",
        "    print(\"get group max feature\")\n",
        "    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n",
        "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
        "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
        "    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
        "    \n",
        "    print(\"get group min feature\")\n",
        "    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n",
        "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
        "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
        "    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
        "    \n",
        "    print(\"get group size feature\")\n",
        "    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
        "    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n",
        "    \n",
        "    print(\"get match mean feature\")\n",
        "    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n",
        "    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
        "\n",
        "    print(\"get match size feature\")\n",
        "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
        "    df_out = df_out.merge(agg, how='left', on=['matchId'])\n",
        "    \n",
        "    df_test_copy = None\n",
        "    if not is_train:\n",
        "      df_test_copy = df_out[[\"Id\", \"maxPlace\"]].copy()\n",
        "    df_out.drop([\"Id\", \"matchId\", \"groupId\"], axis=1, inplace=True)\n",
        "\n",
        "    X = np.array(df_out, dtype=np.float64)\n",
        "    \n",
        "    feature_names = list(df_out.columns)\n",
        "\n",
        "    del df, df_out, agg, agg_rank\n",
        "    gc.collect()\n",
        "\n",
        "    return X, y, feature_names, df_test_copy\n",
        "  \n",
        "x_test, _, _, df_test_copy = feature_engineering(False)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "pred = model1._predict(x_test)\n",
        "df_to_submit = transform_preds(df_test_copy, pred)\n",
        "print('Saving...')\n",
        "# print(df_to_submit[:5])\n",
        "\n",
        "path = '/content/drive/My Drive/pubg/tanh4x/'\n",
        "save_for_submission(df_to_submit, path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing test.csv\n",
            "remove some columns\n",
            "get target\n",
            "get group mean feature\n",
            "get group max feature\n",
            "get group min feature\n",
            "get group size feature\n",
            "get match mean feature\n",
            "get match size feature\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx4vFuU8k1AW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}